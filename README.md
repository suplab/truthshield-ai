# ✅ Deep Fake Detection & Ethics Advisor

**Objective:** Autonomous agent that detects AI-generated content, flags potential misinformation, and produces explainable reports.

## MVP Features:

- Upload image, video, or text content.
- Detect if content is AI-generated.
- Assess potential impact (e.g., political, social, financial).
- Generate a human-readable report with risk scoring.

## Agentic Workflow:

1. User uploads media → Agent extracts features (frames, metadata, text).
2. Agent runs AI detection models (image/video: DeepFake detection; text: LLM content classification).
3. Agent cross-references content context (social media trends, news events) for risk scoring.
4. Agent generates a report with explanations and recommendations.
5. Optional: Agent flags suspicious content on social media using APIs.

## Tech Stack:

**Frontend**
- Gradio — rapid UI for uploads and demo.

**DeepFake Detection**
- MesoInception4 (MesoNet) — lightweight CNN for image / frame-level DeepFake detection.
- OpenCV — frame extraction from videos.
- Pillow / torchvision transforms — image preprocessing.
- PyTorch — model definition & inference.

**Text Detection & NLP**
- Hugging Face `transformers` — zero-shot text classification (`facebook/bart-large-mnli`) for AI-generated text detection (MVP).
- Option to replace with supervised text classifier later.

**LLM / Report Generation**
- Ollama (local) with `llama2` — merges detection summary + narrative report (subprocess invocation).

**Storage / Optional**
- Local filesystem for MVP; later: vector DB (e.g., Pinecone, Milvus, or Weaviate) for fingerprints / indexing.

**Deployment / Ops**
- Python 3.8+, pip, optionally Docker.
- (Optional) GPU + CUDA for faster inference.


## Architecture Overview

```
User Upload (Image / Video / Text)
            │
            ▼
   Media Type Detection
            │
  ┌─────────┴─────────┐
  │                   │
Image/Video          Text
Detection             Detection
(DeepFake Model)     (LLM/Text Classifier)
  │                   │
  └─────────┬─────────┘
            ▼
      Context Analysis
  (Trends, News, Social Impact)
            │
            ▼
   Risk Scoring & Explanation
            │
            ▼
     Generate Report (GPT-4)
            │
            ▼
        Gradio UI
```

## Project Structure

```
truthshield-ai/
├── README.md
├── requirements.txt
├── app.py
├── models/
│   └── placeholder_models.py
├── utils/
│   ├── media_utils.py
│   ├── detection_utils.py
│   └── risk_utils.py
├── examples/
│   ├── sample_image.jpg
│   ├── sample_video.mp4
│   └── sample_text.txt
└── config.env
```

## Quick Start

### 1. Prerequisites

- Python 3.8+  
- [Ollama installed locally](https://ollama.com/docs) and model `llama2` pulled:
  ```bash
  # install ollama (follow https://ollama.com/docs)
  # pull a model, e.g. llama2
  ollama pull llama2
  ```
- (Optional but recommended) GPU and CUDA for faster inference.

### 2. Install Python dependencies

```bash
git clone <repo_url>
cd truthshield_ai
python -m venv .venv
source .venv/bin/activate   # Linux/macOS
.\.venv\Scripts\activate  # Windows
pip install -r requirements.txt
```

### 3. (Optional) Auto-download MesoNet weights

Place the MesoNet checkpoint at `models/meso_inception4.pth`.
You can set the env var `MESO_WEIGHTS_URL` to a direct download link and run the helper:

```bash
export MESO_WEIGHTS_URL="https://your-host/path/meso_inception4.pth"
python models/download_weights.py
```
If you don't provide weights, the app will still run but detection will use randomly initialized MesoNet (useful for testing pipeline only).

### 4. Run the app

```bash
python app.py
```
This will launch a Gradio web UI. Upload one file (image/video/text) and optionally enable the "Generate merged Ollama report" toggle. The app will return a single text block that contains a detection summary followed by a narrative report generated by `llama2` via Ollama.

## Notes

- **Detection model:** `utils/detection_utils.py` contains a compact MesoInception4 variant for fast inference. For production-level accuracy use well-trained weights (Meso or Xception) and consider fine-tuning.
- **Weights:** The repo intentionally uses a helper `models/download_weights.py` that will download weights if you set `MESO_WEIGHTS_URL`. This avoids hardcoding external links; instead you can host weights on your own server or S3 and provide the link.
- **Ollama:** The app calls `ollama run llama2` via `subprocess.run`. Ensure `ollama` is installed and llama2 is available locally (`ollama pull llama2`).
- **Text detection:** Uses a Hugging Face zero-shot classifier (`facebook/bart-large-mnli`). For better reliability, consider training a supervised classifier on human vs AI text datasets.
- **Risk heuristic:** A simple risk score is added (confidence * weight). This is a placeholder — replace with more sophisticated context-aware scoring (social impact, named entities, cross-referencing news) for production.

## Next steps (Roadmap)

1. Integrate a vector DB to store content fingerprints (for duplicate detection / history).
2. Add social media context enrichment (Twitter/X, Reddit, etc.) to compute wider impact.
3. Replace/augment MesoNet with Xception or an ensemble for improved accuracy.
4. Add user authentication, audit logs, and exportable PDF reports.
5. Add a web-service deployment (Dockerfile + cloud infra).